{"blogs/2025-building-with-ai":{"slug":"blogs/2025-building-with-ai","filePath":"blogs/2025-building-with-ai.md","title":"AI, Vibe Coding & The Joy of Building","links":[],"tags":[],"content":"It’s 2026, and we are just seeing the top-level effects of AI unfold. It is barely a year since Claude Code, Amp, and others were launched, and boy, how these tools have shaped the way I did my job over the past year.\nI have been experimenting with vibe coding since around September 2024 (if you were to include the tab auto-complete which came out with GitHub Copilot, then I don’t remember the year, but since then). I started out with Cursor like most folks (I guess?), and again like most, I didn’t like what it was doing. It made me slower rather than giving me a performance bump, resulting in me dumping it altogether.\nI went back to it a couple more times on and off and didn’t like it until Jan/Feb 2025. One weekend, I was just sitting idle, getting bored, and thought to myself, “Let’s build a side project we have been putting off for a long time.” I opened up Cursor in an attempt to try and use the tool properly this time, since I actually had some time on my hands rather than the previous couple of times where I just wanted it to help me ship something faster without thinking much about what a tool like this would look like.\nI started out with an attempt that I’ll program most of it. I would read a lot since I hadn’t had a lot of experience in building the tool I was building, and I would use Cursor to learn better and help automate smaller things like bootstrapping the project, etc.\nTo my surprise, the project had improved quite a lot (and also my intuition on how to use it as well, thanks to Twitter). It gave me good results in the small directional prompts that I gave it without needing much hand-holding. I think a lot of this is attributable to the model improving as well, since the models were also improving rather exponentially at that time. As a result, I was able to get a very crude (made for learning purposes) version of Veil up and running and even set up a proper homebrew installation for it—something I had never done before, nor did I have to think about it much.\nThis was my first encounter with understanding the power of these AI-assisted coding tools for the first time, and I was absolutely in awe. I felt the sort of joy that I had felt the first time I created a QBasic program back in 6th grade—the feeling of magic, the feeling of sensing the vast potential it gave me. The same raw joy overpowered me, and ever since then, I kept coming back to it.\nThe codebase at work and the tools were not mature enough to leverage it at work yet, but even then, I kept coming back to it for Veil and for trying to leverage it to build some other things. As a normal course of progression, I tried to get it to do more and more. As a result, it started sucking again, since it was not very good at handling long-running tasks at that time (and also felt like a token sink to me).\nThis went on for the next 4-5 months. During this time, Amp came out, and they very generously gave us some free credits. There was a lot of hype around it, and I am always keen to try it out, so I gave it a spin. Boy, oh boy, did it impress me. It helped me set up a 7-year-old legacy Python monolith in mere minutes and debugged the issue I was working on completely. However, this tool was too expensive for me to sustain, and hence I went back to Cursor and VS Code Copilot.\nThen I laid my hands on Claude Code, and boy, I was blown away. This tool pushed me out of the IDE in such a natural way. It was able to understand what I was trying to tell it—a legit feeling of someone who just gets it. It was like finding that one co-worker you can just talk to in one sentence, and they just understand the entire context of how you are trying to engineer something.\nIt helped me ship so much at work, all the while rarely opening up an IDE. Sure, there were casual sessions where it messed things up, and I had to spend time debugging what it was doing and ask it to make smaller edits. But those instances also made me realize that at the end of the day, it’s not a real human but a tool, and I should steer it correctly. It’s much like a car which can get me from home to work much faster than running or walking, but needs me to keep steering it in the right direction at the right time, else it would just cause a massive accident. It was sort of the same realization, and I focused on improving my workflows, planning, and the way I steered and orchestrated the tasks. In some ways, it helped me think more in terms of a manager—the same analogies of how I would get a fresh-out-of-college average intern to help me out in my day-to-day. I tried applying the same principle to working with Claude, and it has served me well.\nThis entire journey of the better part of 2025 has helped me understand the exact difference between “vibe coding” something and actually doing AI-assisted development. The latter is for serious work for an actual engineer at work, and the former is for someone just trying to have fun, trying to hack together a simple dashboard, or a script to assist you with things that are trivial and low-reward for the effort of actually sitting and coding out by your own hands. But if someone were to spit those out reliably in mere minutes, they add a lot of value in speeding up your workflow. A simple dashboard which sanitises the task_ids to be used between different tools like your Axiom log stream and the Datadog trace, or building a visualiser for rendering a transcript in a particular visual format for easy viewing, or just simply stringing together a small web page for showcasing the results and summary of the week-long sprint to the CTO of your company.\nAll the above are small, low-blast-radius, good-to-have use-cases where neither the quality of code matters nor the structure because these are use-and-throw. An example of this is this tweet by Mitchell (the creator of Ghostty).\nAI-assisted development, on the other hand, is using AI to save on time and bridge the gap between your knowledge, understanding, and execution. It means taking ownership of the code; the AI can generate it, implement your ideas for you, but it might not make the same decisions as you did. As a programmer, you need to be able to foresee what it is going to do wrong or might do wrong. You might not be able to do that completely, obviously, but as you learn with your own experience, you understand those patterns and you need to account for them.\nAI-assisted development means reading through the code it has generated because it’s your code at the end of it all. Even if you get your tool to commit code, it says that it was Co-Authored by it (Co being the keyword here). AI cannot and should not be held responsible for the output or the bugs that might come with the unverified and unreviewed code the AI generated. That onus still, and for a foreseeable future, will remain on the engineer working with it. This is a singular understanding which also makes me feel that my job just got better and expanded its scope rather than being on the verge of going extinct or being taken away by a machine/tool. Sure, if you don’t have the love of the craft, don’t have attention to detail or taste, and you just focused on pushing Jiras on a board, you are right to be concerned. Someone who has a much higher bar for the worth of their time will not settle for your mediocrity anymore and would just get an agent to do those things because you were nothing more than that—just an older version of the agent like Cursor was with me back in 2024 when I couldn’t get it to do things. The only difference being before the advent of LLMs, these human agents were required for these high-agency, love-of-the-game programmers/engineers/builders to delegate the offbeat grunt tasks. But with AI, that constraint has been partially removed and will keep getting smaller and smaller as people come to understand that the output which AI generates is nothing but a reflection of what you would have generated yourself given enough time to generate it. What the tool did is just compress and condense the timeline of getting that output out and showing it’s results, if you care about what you are shipping, quality of your work,  you will always ensure that no-matter who did the implementation it meets the exact standards of quality that you have in your mental model. Infact these tools help you ensure those standards are met and tracked rather easily because the manual labour of setting up those guardrails and feedback loops and most of all maintaining them, could easily be delegated off to the AI agent.\nFor me personally, the builder in me has never been happier, and also this experience has been a grounding one. It has made me see my strengths and also my weaknesses—where I lack, where I need help, and where my execution is flawless. One such area is not sticking to things long enough to finish them properly. I have started a couple of projects with LLMs and agents and that has helped me learn a lot technically, but it has also made me realize that I haven’t finished and shared anything completely and properly with the world. That balance between perfectionism and being clean &amp; usable enough to ship and iterate on things is missing, but now that it has been identified, I am working towards closing the gap real fast.\nThis is such a great time to be working in this industry—lots of great products coming out, lots of innovation happening, and the landscape changing at godspeed in realtime is just so exciting. Software engineering and building with code has always felt like magic to me. The magical feeling is what has drawn me towards it since always, and this entire experience is just really, really magical and makes me feel so alive. The joy of building and having a tool which can help me build at the speed of thought—given I know how to instruct it properly, how to review the output properly, and understand where to be extra careful and where to let it slop it out—is just amazing. I can now utilise the time I spend travelling to and from work without guilt because all I need to do is just talk to my phone with Whisper Flow, and I know an agent is working to execute my will while I sit in a car talking to my girlfriend, listening to music, sleeping, or just simply thinking about what next I can do.\nIf you get to work with this technology, I seriously hope you learn to master it and develop a love of the craft because being at the centre of this is such a privilege."},"blogs/index":{"slug":"blogs/index","filePath":"blogs/_index.md","title":"Blogs","links":[],"tags":[],"content":"I have been trying to get into the habit of writing for a while now and after experimenting with the journalling for a while, I think it’s time for me to start sharing my thoughts over here, just so I can document my journey as it’s still just the start of it.\nPosts mostly would revolve around what I think about technology, AI and whatever is on my mind.\nDisclaimer All the thoughts presented in these blogs are personal and my own and they in no way represent my employers. It’s a personal internet blog for me to share what I think."},"grimoire/ampp/10":{"slug":"grimoire/ampp/10","filePath":"grimoire/ampp/10.md","title":"10","links":[],"tags":[],"content":"Queues, memory management and the ABA problem\nPools\n\nA pool is similar to a Set with the following two distinctions:\n\nmight not have a contains() method for testing membership\nallows duplicate elements\n\n\nPools usually act as buffers in producer&lt;&gt;consumer systems.\nTypes of Pool:\n\nBounded:\n\nNumber of items == capacity\nUseful when we want to limit the consumer lag.\n\n\nUnbounded\n\nUnlimited number of items\nUseful when we can’t fix a upper cap on the consumer lag.\n\n\n\n\n\nQueues\nKeywords\n\nLinearizability\nSentinel Node\nAtomicInteger\n\nReferences\n\nWhat is Linearizability\n"},"grimoire/ampp/index":{"slug":"grimoire/ampp/index","filePath":"grimoire/ampp/index.md","title":"AMPP","links":[],"tags":[],"content":"Notes from the book The Art of Multi Processor Programming"},"grimoire/ddia/3":{"slug":"grimoire/ddia/3","filePath":"grimoire/ddia/3.md","title":"3 - Storage & Retrieval","links":[],"tags":["notes","books","ddia"],"content":"\n\nSorted String Tables (SSTables)\n\n\nMaintaining sorted order of data is simpler in memory, we can use any balanced tree (Red-Black, AVL etc.) which will allow us to insert the data in any order but read it back in a sorted order always.\n\nThese in-memory trees are called memtables\n\n\n\nLog Structure Merge Trees (LSM Trees)\n\nStorage engines that work on this principle of merging and compacting stored files are called LSM based engines\nLucene the indexing system used in ElasticSearch is also based on LSM Trees where the index (which is the word) is the key and the value is the list of IDs of documents where that word appears, these indexes are compacted and merged in the BG as and when required.\nLSM Trees are inherently slower in searching for keys that don’t exist (they’ll search for everything and then say nothing exists — this is where the Bloom Filter shines since despite being a probablistic DS it can be sure of negative set memberships.)\nLSM Trees are write optimised\n\n\n\nB-Trees\n\nMost commonly used Index Implementations in Relational DBs\nB-Trees have a very good distinction from the LSM Trees and that is the process of overriding pages, since BTrees mostly store pointers to pages, they override the page without changing it’s reference keeping the structure intact and searches reliable, an LSM Tree on the other hand just appends merges and deletes, i.e. it changes page references but never overrides\nA 4 level BTree with 4KB page size can store 256TB of data\nWrite Ahead Log (WAL) is used to make databases resilient to crashes\n\nWhat you do in a write ahead log is just write every operation before applying it to the tree — this makes sure that if the DB/System crashes mid update you can construct it back up\nthis is similar to how we can fix a memtable as well, we maintain a append only file for a memtable from which it can be reconstructed in case of a failure\n\n\nB-Trees are Read Optimised\n\n\n\nSecondary Indexes\n\nIndexes is the key which a query searches for\nB-Trees or LSM Trees can be used to implement them\nThe result could either be the row itself or it could be a reference to a heap file which helps to reduce data duplication incase of multiple indices and keep the data in a single place\nclustered index help prevent the hop from heap file locations by storing the row directly\n\n\n\nTransaction Processing\n\n\nOnline Transaction Processing (OLTP) - Where the user queries some data and gets the entire row back via some index\n\n\nOnline Analytics Processing (OLAP) - Where the user queries some data to read and aggregate a couple columns\n\n"},"grimoire/ddia/6":{"slug":"grimoire/ddia/6","filePath":"grimoire/ddia/6.md","title":"6 - Partitioning","links":[],"tags":[],"content":"\nshared-memory architecture\nshare-disk architecture\nshared-nothing architecture — horizontal scaling or scaling out\n"},"grimoire/ddia/index":{"slug":"grimoire/ddia/index","filePath":"grimoire/ddia/index.md","title":"DDIA","links":[],"tags":[],"content":"Notes from the book Designing Data Intensive Applications by Martin Kleppmann."},"grimoire/go/go-prog-lang":{"slug":"grimoire/go/go-prog-lang","filePath":"grimoire/go/go-prog-lang.md","title":"Go - Programming Language","links":[],"tags":[],"content":"Chapter 1\n\ngo has no notion of enclosing conditions in (\nthe built in function make can be used to create a new map\nThe order of map iteration in go is random varying from program run to run\nThe map is actually a reference to a data-structure created by make\nReadFile returns a byte slice\nhttp. returns a struct (expected)\nresp.body is a stream\n"},"grimoire/go/index":{"slug":"grimoire/go/index","filePath":"grimoire/go/index.md","title":"Go","links":[],"tags":[],"content":"Notes from my knowledge of the go programming language."},"grimoire/index":{"slug":"grimoire/index","filePath":"grimoire/index.md","title":"Grimoire","links":[],"tags":["notes"],"content":"Welcome to my Grimoire. I find programming and software engineering to be wizardy hence the name of my vault for notes is Grimoire.\nThis is the place where all my technical learnings and notes are stored, I initially had a scattered obsidian vault for them, but then one day I came across quartz and thought, why not make the notes public atleast that way I’ll be able to hold myself more accountable to write more cleanly and also build an online presence and help those like me who are looking for resources but have to find their way through numerous different resources scattered all across the web."},"grimoire/reading-list":{"slug":"grimoire/reading-list","filePath":"grimoire/reading-list.md","title":"Reading List","links":["grimoire/ddia/","grimoire/go/go-prog-lang","grimoire/ydkjsy/"],"tags":["notes","research"],"content":"List of the books, research papers, articles, blogs etc. that I have read or plan on reading.\nTechnical Books\n\nDesigning Data Intensive Applications (DDIA)\nThe Go Programming Language\nYou Don’t Know JS Yet (YDKJSY) the book series\nConcurrency in Go\n"},"grimoire/ydkjsy/index":{"slug":"grimoire/ydkjsy/index","filePath":"grimoire/ydkjsy/index.md","title":"YDKJSY","links":[],"tags":[],"content":"Notes from the awesome book series You Don’t Know JS Yet by Kyle Simpson"},"grimoire/ydkjsy/promises":{"slug":"grimoire/ydkjsy/promises","filePath":"grimoire/ydkjsy/promises.md","title":"Promises","links":[],"tags":[],"content":"\n\nThink of programs as “chunks” - one of these chunks execute “right now” and others execute “later”.\n\n\nSimplest “chunk” can be a function\n\n\nUntil ES6 JS had no internal notion of asynchrony\n\nJS Engine runs on a “hosting env”\nThe hosting env has the “event loop”\nFor example: in case of AJAX requests, it is the ‘browser’ who listens for the response of the network call\n\n\n\nevent loops thinks and talks in terms of “tasks”\n\n\nDifference between asynchrony\n\nEvent loop is pretty different from\nRun To Completion - JavaScript has “run to completion” behaviour, which means that if one function starts executing, then only it’s code will run to completion before anything else runs\nEvent loop might introduce non-determinism in the code but due to “event” ordering (this is due to the single threaded) nature of things and not because of “inter-leaving” of statements.\n\nThis is what a simple “race condition” is.\n\n\nIf the processes are “non-interacting” then non-determinism is perfectly acceptable\n\nCafeteria student example\n\n\n\nJobs\n\n“Job Queue” on top of event loop — used to schedule async tasks inside async tasks - but why?? because these task are a part of the current async task (do later task) but they still need to be finished before other do later tasks\n“Job Queue” hangs off of each item in the event loop — i.e. each item in the event loop has a job queue associated to it\nAlways remember that the “job queue” is associated with the current task of the event loop\n\nKey terms to note\n\ntick (of an event loop) -  Each iteration of the event loop is a tick\nprocess\ntask\nevent - async function invocations\n“latch” conditions\n\nChapter-2: Callbacks\n\nEvent Loop aligns well with how humans think of things\nCallback hell\nLack of sequentiality and alignment with human mental model\nLack of trust — specially with “inversion of control” involving external parties\n\nkey terms\n\nZalgo effects\n\nChapter-3: Promises\n\nPromises help prevent “inversion of control”\nBecause Promises encapsulate the time-dependent state—waiting on the fulfillment or rejection of the underlying value—from the out‐ side, the Promise itself is time-independent, and thus Promises can be composed (combined) in predictable ways regardless of the tim‐ ing or outcome underneath.\nResolved promises are immutable\nPromises are identified using thenable duck typing\n\nthenable: is anything that has a “then” property on it which is a function\nduck typing: is The general term for type checks that make assumptions about a value’s type based on its shape (what properties are present)\n\n\n!!!! Very Important to Node !!! then(...) calls themselves return promises\n\nTrust with Promises\n\nThey prevent the zalgo effect inherently.\n“Jobs” (queues hanging off of tasks in the event loop) are at the core of Promises making them more predictable especially in terms of early or late firing of callbacks passed to “then” or “catch”\n“race” can be used to define that a promise is not getting resolved or has hanged\n\nkey terms\n\nthenable\nduck typing\n"},"grimoire/ydkjsy/this-and-prototypes":{"slug":"grimoire/ydkjsy/this-and-prototypes","filePath":"grimoire/ydkjsy/this-and-prototypes.md","title":"this and Object.prototype","links":[],"tags":[],"content":"\ndefault binding cares about the “strict” mode of the function and all other care about the “strict” mode of the call-site (boom boom moment)\n"},"index":{"slug":"index","filePath":"index.md","title":"~/techsavvyash","links":["projects/BSAI","projects","blogs","grimoire"],"tags":[],"content":"I’m a software engineer trying to build systems which make people’s lives easier. I have 3 years of experience working on production systems for a couple of Indian states helping the under privileged strata of the population thanks to the amazing opportunities and problems I got to work on during my time at Samagra. Presently, I ship code at Zomato trying to simplify the lives of merchants on the platform.\nI was part of the core team that built Bharat Sah’AI’yak which was acquired by OLA Krutrim in early 2025. BSAI is a AI enabled compute platform which could be modified for a wide range of usecases like ChatBots, information processing systems etc.\nI have a deep love for databases, distributed systems, PLT and open source. Currently, I am exploring making AI coding agents smarter and simplifying selling &amp; hosting APIs on the internet with my friends over at TheFlywheel - Boutique AI Studio.\nSince you are here, you might want to take a look at my work and also blogs once I start publishing them. Oh, and I also maintain my reading notes on this website."},"projects/BSAI":{"slug":"projects/BSAI","filePath":"projects/BSAI.md","title":"Bharat Sah'AI'yak","links":[],"tags":["projects"],"content":"Bharat Sah’AI’yak or BSAI is a AI enabled compute platform aimed to be flexible enough to serve a varied variety of usecases from building RAG based chatbots (like AKAI, KMAI, MahaKumbh bot etc. - more on those in these later), data processing pipelines, testing suite for LLM response accuracy and a lot of other use cases -  since it is a compute platform was designed to behave more like a programming language on steroids.\nIn the early days the development, BSAI was adopted for Agriculture with a singular problem statement - getting the crop and cattle farming advisory at the finger tips of farmers keeping in mind their troubles with technology.\nThe BSAI team came up with just the right fit of solution which given the nature of business at Samagra was scalable and adaptable enough to be deployed in context of all the states where we were running agricultural advisory programs.\nThe project ran a successful pilot in two indian states:\n\nOdisha - Under the name of AmaKrushiAI chatbot (AKAI)\nUttar Pradesh - As KisanMitra AI (KMAI)\n\nAs part of improvements for these two production deployments being used by around 50K farmers per day we added auto training pipelines to keep the chatbots updated with latest agricultural advisory data which included of writing our own PDF parsers, table parser because the advisory from Indian meterological institutes of these state are in the form of printed textbook pdfs.\nAfter this BSAI was also used to build the chatbot for Maha Kumbh 2025 as a part of Government of India’s Digital India and Digital Kumbh initiatives where millions of devotees used the bot for a period of the festival and we served them seamlessly. Another thing which was special about MahaKumbh BSAI adoption was that it was the first time where external teams like OLA Krutrim and BeeHyv used and built on top of BSAI proving that the platform was mature enough to onboard external teams to just use the developer platform and build on top of it to power population scale use cases.\nBSAI was ultimately acquired by OLA Krutrim in early 2025 and is currently being worked on as part of Krutrim’s larger vision of building India’s own state of the art AI agent."},"projects/Beckn":{"slug":"projects/Beckn","filePath":"projects/Beckn.md","title":"Beckn","links":[],"tags":["projects","gov-tech","open-source"],"content":"Beckn is a universal resource discovery and transaction protocol that enables the leap to open, decentralized, and interoperable peer-to-peer networks. Beckn is a great choice when you want to build projects and products integrated various different providers into a singular discovery platform. At it’s core Beckn has three main network participant entities - Beckn Gateway (BG), Beckn Application Platform (BAP) and Beckn Provider Platform (BPP). In very simplistic and partially right terms these could be thought of simply as a network level proxy or gateway server connecting a client side application (BAP) to a server side application (BPP). I will write a more detailed blog around Beckn sometime later, till then this much information is enough to understand more about the projects I have worked on with Beckn.\nMy first ever production project that I worked on was the loan discovery platform for Odisha Government’s SAFAL Portal. The problem statement was one click enablement of credit for farmers from various credit providers since the previous process for getting credit was very cumbersome and unapproachable for the technologically challenged farmers. We employed Beckn to allow for this by writing an adaptation of the specification for credit domain and then developing a system where multiple banks could be onboarded without much overhead of development on their pre-existing systems by developing simple adapters which could just transform the payloads from the bank’s backend specifiation to Beckn specification and vice-versa. The entire codebase is open sourced in the Konnect Odisha github organisation. The stack features a DEPA based consent manager to store the user’s consent for what information they want to share with the credit providers and what information they don’t.\nIn the system the safal portal acted as a BAP and the credit providers acted as the BPPs all of which could be accessed on the platform and the entire process and stage of the application could be tracked from there to enable easy and seamless integration making the process of seeking credit a lot more simpler for the farmers.\nThe second instance was when we joined the Beckn community to enable Open network for learning and livelihood transactions (ONEST) which is the adaptation of Beckn for the skilling and education domain. As a part of the community we helped design and shape the ONEST specification and also helped with a reference implementation for the initial v1.1 of the specification which was launched and used in for the Skillathon organised by FIDE to introduce ONEST with the world when it was in it’s early stages of development in Feb’23. ONEST was known as Decentralised Skilling and Education Protocol (DSEP) protocol. The reference implementation can be found at the samagra-development/dsep github."},"projects/OIDC":{"slug":"projects/OIDC","filePath":"projects/OIDC.md","title":"OIDC Lite","links":[],"tags":["projects","open-source"],"content":"OIDC Lite is a project I built with my college friends Debatreya, Utkarsh &amp; Ashutosh to learn more about the OIDC protocol and also because we wanted a lighter alternative to FusionAuth and KeyCloak which are great feature rich projects but for most usecases we don’t need those features and while rolling your own auth is fun, it gets boring and repeatitive after a couple of times, hence as a result, OIDC light was born, which could be used as a authentication solution similar to FA and Keycloak but without the bloat.\nYou can read more about the project in the OIDC Lite docs."},"projects/RCW":{"slug":"projects/RCW","filePath":"projects/RCW.md","title":"Verifiable Credentials","links":[],"tags":["projects"],"content":"Introduction\nRegistry, Credentials and Wallet (RCW) is a project that implements the core Verifiable Credentials (VC) Specification published by W3C.\nInitial implemenetation of RCW was done as the core credentialling services for the Unified Learner’s Passbook (ULP) project which was proposed in the National Education Policy, 2020 (NEP 2020) by Ministry of Education, Government of India. The pilot launch of ULP is scheduled for the state of Uttar Pradesh, India (most populous state of India).\nOpen Source Relevance\nThe core services, namely, Credential-MS, Cred-Schema-MS and Identity were abstracted out and contributed to Sunbird RC. Sunbird RC is the software which powers the generation and management of credentials like our COVID Vaccination certificate in India. These core services are maintained by the original implementers, SamagraX alongside me here at Registry, Credentials and Wallet (RCW).\nLink to commits contributed by me to Sunbird RC for their 2.0 release\nArchitecture, Tech Stack and Deployment\nThese services follow a microservices architecture to enable credentialling in your project. The architecture of how these services work with each other is shown in the picture below.\n\nEach of these services are built using NestJS with TypeScript and come bundled with Dockerfile, Jenkinsfile and Ansible roles for easy deployment. The RCW Github Organisation has the devops which hosts a one click deployment mechanism of these services to be used in any project.\nReferences\n\nVerifiable Credentials Specification\nNational Education Policy 2020\nUnified Learner’s Passbook Github\nSunbird RC\nTeam SamagraX on Github\nRCW Github\n"},"projects/Stencil":{"slug":"projects/Stencil","filePath":"projects/Stencil.md","title":"Stencil","links":["projects/BSAI"],"tags":["projects","open-source","frameworks"],"content":"Stencil is yet another JavaScript framework built on top of NestJS which is another backend JavaScript framework inspired by AngularJS. Stencil was developed to increase the productivity of engineers at SamagraX so that they spend less time setting projects up and more time writing business logic hence stencil comes with a lot of prebuilt components which can be setup with a single command hence in a way stencil is just NestJS on steriods right now.\nThe roadmap for stencil is to get out of the JavaScript ecosystem and become a standalone language agnostic and AI enabled framework easing developer’s lives as part of DoughHQ which is a side project I started post the initial engineering team of SamagraX exited after the Bharat Sah’AI’yak landing with Krutim. Any and all contributions are welcome.\nThe list of components that presently come out of the box with stencil are:\n\nAuthentication setup using our internal User Service.\nRate limiting and GeoIP interceptors\nIntegration with temporal\nSupport for config driven application bootstrapping which was very graciously contributed by Savio Dias as a part of his C4GT’24 project.\nObservability using the Prometheus and Grafana with single click setup and pre-built dashboards directly via the APIs.\nAuto generation of OpenAPI specification for the project.\n\nMore cool stuff (stencil roadmap) and AI enabled development will be coming soon to Stencil as I start getting more time to work on the project."},"projects/Stylus":{"slug":"projects/Stylus","filePath":"projects/Stylus.md","title":"StylusDB","links":[],"tags":["projects","open-source","databases"],"content":"StylusDB is a collection (presently 2 but there will be more) of toy databases built by me and Chakshu Gautam while trying to tinker around and learn the database internals.\nThere is Stylus-SQL which is an implementation of a small SQL DB with it’s own SQL parsing engine and storage layer and everything. This was created in a tutorial like fashion so that people (mostly college students) can start taking up building their own database without being very overwhelmed and we started with a very crude regex and csv file based implementation so make it as approachable as possible but suggestions and improvements are always welcome.\nWe also took a couple of live sessions for the first 21 steps recordings for which can be accessed as this Youtube Playlist.\nWe also have StylusDB JS (there is no fancy name for this one yet) which is a redis like distributed key value store that I built as a part of my B.Tech. final year project. This was aimed at me trying to learn more about distributed systems in a practical way after taking a theory course on distributed computing in my 7th semester and as a result of this I researched into and read about distributed consensus algorithms like Paxos, RAFT, ZAP, etc. and finally ended up using RAFT to implement the distributed databases trying to opitmise and modify the algorithm to make it a bit faster and I ended up implementing things like direct reads from followers rather than only the leader to serve the requests and reduce response times with the trade off being that the data might be stale due to eventual consistency. We also used SIMD for faster TCP stream data processing trying to shave off some time and we did see around 1.5-2x perf boost over the native JSON.parse().\nThe architecture for the project looked something like shown below.\n\nStylusDB was started along side a small community called Status 20X which I aim to create as a place for deep tech nerds who can think beyond passing tech interviews and actually enjoy the computer science."},"projects/Veil":{"slug":"projects/Veil","filePath":"projects/Veil.md","title":"Veil","links":[],"tags":["products","open-source"],"content":"Veil is a modded caddy fork which gives API paywalls as a built in feature to the caddy webserver running in a reverse proxy mode. Veil aims to be a self hostable solution for those who want lesser number of features or want to tweak it as per yours wishes with a fully featured SaaS marketplace coming soon to ease your deployments and paywall management.\nBelow is a small demo for Veil.\n\nPlease refer to Veil docs to know more about the project."},"projects/index":{"slug":"projects/index","filePath":"projects/_index.md","title":"My Work","links":[],"tags":[],"content":"I started programming in 2020 during CoVID-19 quarantine lockdown right after I graduated high school and appeard for JEE. I had a little bit of experience programming in python and writing HTML pages from my school days because those lectures on QBasic, HTML and Python and how it all seemed like wizardy at that time to a 12 year old is what gave birth to my love and passion for my craft.\nYou can read more about some of the bigger projects I have worked on from the list below."}}